---
layout: ../layouts/Layout.astro
title: "ActSafe: Active Exploration with Safety Constraints for Reinforcement Learning"
description: How to explore dynamic environments safely?
favicon: /favicon.svg
thumbnail: /screenshot.png
---

import { Image } from "astro:assets";

import Layout from "../layouts/Layout.astro";

import Header from "../components/Header.astro";
import TwoColumns from "../components/TwoColumns.astro";
import Video from "../components/Video.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import PDF from "../components/PDF.astro";
import Figure from "../components/Figure.astro";
import LaTeX from "../components/LaTeX.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Splat from "../components/Splat.tsx"

import CodeBlock from "../components/CodeBlock.astro";
export const components = { pre: CodeBlock }

import outside from "../assets/outside.mp4";
import transformer from "../assets/transformer.webp";

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Yarden As",
      url: "https://yas.pub",
      institution: "ETH Zurich",
      notes: ["*"],
    },
    {
      name: "Bhavya Sukhija",
      url: "https://sukhijab.github.io/",
      institution: "ETH Zurich",
      notes: ["*"],
    },
    {
      name: "Lenart Treven",
      url: "https://lenarttreven.github.io/",
      institution: "ETH Zurich",
    },
    {
      name: "Carmelo Sferrazza",
      url: "https://sferrazza.cc/",
      institution: "UC Berkeley",
    },
    {
      name: "Stelian Coros",
      url: "https://crl.ethz.ch/people/coros/index.html",
      institution: "ETH Zurich",
    },
    {
      name: "Andreas Krause",
      url: "https://las.inf.ethz.ch/krausea",
      institution: "ETH Zurich",
    },
  ]}
  conference="Preprint"
  notes={[
    {
      symbol: "*",
      text: "Equal Contribution",
    },
  ]}
  links={[
    {
      name: "Code",
      url: "https://github.com/yardenas/actsafe",
      icon: "mdi:github",
    },
    {
      name: "arXiv",
      url: "",
      icon: "academicons:arxiv",
    },
  ]}
  />

<Video source={outside} />

## Abstract

Reinforcement learning (RL) is ubiquitous in the development of modern AI systems. However, state-of-the-art RL agents require extensive, and potentially unsafe, interactions with their environments to learn effectively. These limitations confine RL agents to simulated environments, hindering their ability to learn directly in real-world settings. In this work, we present ActSafe, a novel model-based RL algorithm for safe and efficient exploration. ActSafe learns a well-calibrated probabilistic model of the system and plans optimistically w.r.t. the epistemic uncertainty about the unknown dynamics, while enforcing pessimism w.r.t. the safety constraints. Under regularity assumptions on the constraints and dynamics, we show that ActSafe guarantees safety during learning while also obtaining a near-optimal policy in finite time. In addition, we propose a practical variant of ActSafe that builds on latest model-based RL advancements and enables safe exploration even in high-dimensional settings such as visual control. We empirically show that ActSafe obtains state-of-the-art performance in difficult exploration tasks on standard safe deep RL benchmarks while ensuring safety during learning.
